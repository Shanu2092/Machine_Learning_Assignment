{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "206f80af",
   "metadata": {},
   "source": [
    "## 1. What is the difference between supervised and unsupervised learning? Give some examples to illustrate your point.\n",
    "\n",
    "        In supervised learning,the end result is known to us,called the labels.while in unsupervised learning,the data is unlabelled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043e3885",
   "metadata": {},
   "source": [
    "## 2. Mention a few unsupervised learning applications.\n",
    "\n",
    "        i. segmentation\n",
    "       ii. Visualisation\n",
    "      iii. Dimentionality reduction\n",
    "       iv. Anomaly detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7066ebf",
   "metadata": {},
   "source": [
    "## 3. What are the three main types of clustering methods? Briefly describe the characteristics of each.\n",
    "\n",
    "        i. Centroid-based :  clustering organizes the data into non-hierarchical clusters, in contrast to hierarchical clustering defined below. k-means is the most widely-used centroid-based clustering algorithm. Centroid-based algorithms are efficient but sensitive to initial conditions and outliers. This course focuses on k-means because it is an efficient, effective, and simple clustering algorithm.\n",
    "        \n",
    "       ii. Density-based Clustering : Density-based clustering connects areas of high example density into clusters. This allows for arbitrary-shaped distributions as long as dense areas can be connected. These algorithms have difficulty with data of varying densities and high dimensions. Further, by design, these algorithms do not assign outliers to clusters.\n",
    "      \n",
    "       iii.Distribution-based Clustering : This clustering approach assumes data is composed of distributions, such as Gaussian distributions.the distribution-based algorithm clusters data into three Gaussian distributions. As distance from the distribution's center increases, the probability that a point belongs to the distribution decreases. The bands show that decrease in probability. When you do not know the type of distribution in your data, you should use a different algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6997c0",
   "metadata": {},
   "source": [
    "## 4. Explain how the k-means algorithm determines the consistency of clustering.\n",
    "        \n",
    "        K-means algorithms use a metric called the \"inertia\" to measure the model's consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd323de",
   "metadata": {},
   "source": [
    "## 5. With a simple illustration, explain the key difference between the k-means and k-medoids algorithms.\n",
    "\n",
    "        K-means attempts to minimize the total squared error, while k-medoids minimizes the sum of dissimilarities between points labeled to be in a cluster and a point designated as the center of that cluster. In contrast to the k -means algorithm, k -medoids chooses datapoints as centers ( medoids or exemplars)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7965e020",
   "metadata": {},
   "source": [
    "## 6. What is a dendrogram, and how does it work? Explain how to do it.\n",
    "\n",
    "        A dendrogram is a diagram that shows the attribute distances between each pair of sequentially merged classes After each merging, the distances between all pairs of classes are updated. The distances at which the signatures of classes are merged are used to construct a dendrogram.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e741b98",
   "metadata": {},
   "source": [
    "## 7. What exactly is SSE? What role does it play in the k-means algorithm?\n",
    "\n",
    "        The SSE is defined as the sum of the squared Euclidean distances of each point to its closest centroid. Since this is a measure of error, the objective of k-means is to try to minimize this value. The purpose of this figure is to show that the initialization of the centroids is an important step.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978217b2",
   "metadata": {},
   "source": [
    "## 8. With a step-by-step algorithm, explain the k-means procedure.\n",
    "\n",
    "        i. Choose the number of cluster k\n",
    "       ii. Select k random points as centroids\n",
    "      iii. Assign all points to the closest centroid\n",
    "       iv. Recompute the centroids of newmy made clusters\n",
    "        v. Repeat steps 3 and 4\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9258b042",
   "metadata": {},
   "source": [
    "## 9. In the sense of hierarchical clustering, define the terms single link and complete link.\n",
    "\n",
    "        In single-link (or single linkage) hierarchical clustering, we merge in each step the two clusters whose two closest members have the smallest distance (or: the two clusters with the smallest minimum pairwise distance). Complete-link clustering can also be described using the concept of clique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ad5da7",
   "metadata": {},
   "source": [
    "## 10. How does the apriori concept aid in the reduction of measurement overhead in a business basket analysis? Give an example to demonstrate your point.\n",
    "\n",
    "        Apriori algorithm assumes that any subset of a frequent itemset must be frequent. Its the algorithm behind Market Basket Analysis. So, according to the principle of Apriori, if {Grapes, Apple, Mango} is frequent, then {Grapes,Mango} must also be frequent. Here is a dataset consisting of six transactions.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
