{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "159a923b",
   "metadata": {},
   "source": [
    "## 1. Recognize the differences between supervised, semi-supervised, and unsupervised learning.\n",
    "\n",
    "\n",
    "        Supervised learning : In supervised learning,the training data that is fed to model contains the desired solutions called labels.\n",
    "        \n",
    "        Unsupervised learning : In unsupervised learning,the training data is unlabelled,the system learns without a teacher.\n",
    "        \n",
    "        Semisupervised learning : In semisupervised learning,the data is partially labelled\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dd7410",
   "metadata": {},
   "source": [
    "## 2. Describe in detail any five examples of classification problems.\n",
    "\n",
    "        1. Customer behavior prediction : Customers can be classified in different buying patterns,e.g classification can be used to predict wehther a customer is likely to buy some products or not.\n",
    "        \n",
    "        2. Document classification : A multinomial classification model can be used to segment documents to its category.\n",
    "        \n",
    "        3. Spam filtering : It is most widely used classification use-cases,it can be used to separate spam messages and non-spam messages\n",
    "        \n",
    "        4. Image classification : One of the most famous image classification techniques CNN use this to identify and classify images.\n",
    "        \n",
    "        5. Web text classification : Classifying web pages / documents into different topics is another classification problem\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a92198",
   "metadata": {},
   "source": [
    "## 3. Describe each phase of the classification process in detail.\n",
    "\n",
    "        Process of classification consists of two phases :\n",
    "\n",
    "       i. Construction of the classifier\n",
    "      ii. Usage of the classifier.\n",
    "        A classifier in machine learning is an algorithm that automatically orders or categorizes data into one or more of a set of “classes.” One of the most common examples is an email classifier that scans emails to filter them by class label: Spam or Not Spam.\n",
    "\n",
    "        The main goal of the Classification algorithm is to identify the category of a given dataset, and these algorithms are mainly used to predict the output for the categorical data. Multi-class Classifier: If a classification problem has more than two outcomes, then it is called as Multi-class Classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0177c2",
   "metadata": {},
   "source": [
    "## 4. Go through the SVM model in depth using various scenarios.\n",
    "\n",
    "        A Support Vector Machine (SVM) is a very powerful and versatile Machine Learning\n",
    "        model, capable of performing linear or nonlinear classification, regression, and even\n",
    "        outlier detection. It is one of the most popular models in Machine Learning, and any‐\n",
    "        one interested in Machine Learning should have it in their toolbox. SVMs are partic‐\n",
    "        ularly well suited for classification of complex but small- or medium-sized datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63800dc7",
   "metadata": {},
   "source": [
    "## 5. What are some of the benefits and drawbacks of SVM?\n",
    "\n",
    "    i. Benefits :     \n",
    "            a. Works for both linear and non linear classification\n",
    "            b. Capable of regression and even outlier detection\n",
    "            c. Memory efficient\n",
    "            d. More effective in high dimensional space\n",
    "            e. Works well when the number of dimensions are greater than number of samples\n",
    "            \n",
    "    ii. Drawbacks : \n",
    "            a. Not suitable for large datasets\n",
    "            b. Prone to noise\n",
    "            c. In cases where the number of features for each data point exceeds the number of                  training data samples,the SVM will underperform.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9493d4d",
   "metadata": {},
   "source": [
    "## 6. Go over the kNN model in depth.\n",
    "        \n",
    "        K-Nearest Neighbour is one of the simplest Machine Learning algorithms based on Supervised Learning technique.\n",
    "        K-NN algorithm assumes the similarity between the new case/data and available cases and put the new case into the category that is most similar to the available categories.\n",
    "        K-NN algorithm stores all the available data and classifies a new data point based on the similarity. This means when new data appears then it can be easily classified into a well suite category by using K- NN algorithm.\n",
    "        K-NN algorithm can be used for Regression as well as for Classification but mostly it is used for the Classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f33c9fb",
   "metadata": {},
   "source": [
    "## 7. Discuss the kNN algorithm's error rate and validation error.\n",
    "\n",
    "        Training error here is the error you'll have when you input your training set to your KNN as test set. Since your test sample is in the training dataset, it'll choose itself as the closest and never make mistake. For this reason, the training error will be zero when K = 1, irrespective of the dataset.\n",
    "\n",
    "        kNN produces predictions by looking at the k nearest neighbours of a case x to predict its y, so that's fine. In particular, the kNN model basically consists of its training cases - but that's the cross validation procedure doesn't care about at all."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a607eab1",
   "metadata": {},
   "source": [
    "## 8. For kNN, talk about how to measure the difference between the test and training results.\n",
    "\n",
    "        KNN can be used for classification — the output is a class membership (predicts a class — a discrete value). An object is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors.\n",
    "\n",
    "        KNN classifier does not have any specialized training phase as it uses all the training samples for classification and simply stores the results in memory. KNN is a non-parametric algorithm because it does not assume anything about the training data. This makes it useful for problems having non-linear data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f273d0df",
   "metadata": {},
   "source": [
    "## 9. Create the kNN algorithm.\n",
    "\n",
    "        The k-nearest neighbor algorithm is imported from the scikit-learn package.\n",
    "\n",
    "        i. Create feature and target variables.\n",
    "       ii. Split data into training and test data.\n",
    "      iii. Generate a k-NN model using neighbors value.\n",
    "       iv. Train or fit the data into the model.\n",
    "        v. Predict the future"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb042703",
   "metadata": {},
   "source": [
    "## 10. What is a decision tree, exactly? What are the various kinds of nodes? Explain all in depth.\n",
    "\n",
    "        Like SVMs,decision trees are versatile Machine Learning algorithms that can perform both classification and regression tasks,and even multioutput tasks,they are powerful algorithms capable of fitting complex datasets.\n",
    "        \n",
    "        There are three different types of nodes: chance nodes, decision nodes, and end nodes. A chance node, represented by a circle, shows the probabilities of certain results. A decision node, represented by a square, shows a decision to be made, and an end node shows the final outcome of a decision path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88af6076",
   "metadata": {},
   "source": [
    "## 11. Describe the different ways to scan a decision tree.\n",
    "\n",
    "        In a decision tree analysis, the decision-maker has usually to proceed through the following six steps:\n",
    "\n",
    "        i.Define the problem in structured terms.\n",
    "       ii.Model the decision process.\n",
    "      iii.Apply the appropriate probability values and financial data.\n",
    "       iv.Solve the decision tree.\n",
    "        v.Perform sensitivity analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4cc1bf",
   "metadata": {},
   "source": [
    "## 12. Describe in depth the decision tree algorithm.\n",
    "\n",
    "        Scikit learn uses the Classification and Regression Tree (CART) algorithm to train Decision Trees(also called \"growing\" trees).The algorithms works by first splitting the traininng set into two subsets using a ingle feature k and a threshold feature tk.The algorithm searches for the pair (k,tk) that produces the purest subsets(weighted by their size).\n",
    "        \n",
    "        once the cart algorithm has successfully split the training set in two,it splits the subsets using the same logic,then the subsets ,and so on,recursively.It stops recursing once it reaches the maximum depth(defined by max_depth hyperparameter),or if it cannot find a split that will reduce impurity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576e9824",
   "metadata": {},
   "source": [
    "## 13. In a decision tree, what is inductive bias? What would you do to stop overfitting?\n",
    "\n",
    "        A. Inductive Bias in Decision Tree Learning :\n",
    "                i. Shorter trees are preffered over long trees\n",
    "               ii. Prefer trees that place high information gain attributes close to the root\n",
    "               \n",
    "        B. Stop overfitting in the decision tree models:\n",
    "                i. Ensembling\n",
    "               ii. Data Augmentation\n",
    "              iii. Data Simplification\n",
    "               iv. Cross Validation\n",
    "               \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0252ae35",
   "metadata": {},
   "source": [
    "## 14.Explain advantages and disadvantages of using a decision tree?\n",
    "\n",
    "        Advantages of decision trees:\n",
    "            i. It is a white box model ,which means that the decisions made by the model are easy to interpret.\n",
    "           ii. Works for both regression and classification tasks.\n",
    "          \n",
    "        Disadvantages of decision trees:\n",
    "            i. Decion trees love orthogonal decision boundaries,which makes them sensitive to training set rotation.\n",
    "           ii. Very sensitive to small variations in the training data.\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36c400d",
   "metadata": {},
   "source": [
    "## 15. Describe in depth the problems that are suitable for decision tree learning.\n",
    "\n",
    "        Appropriate Problems for Decision Tree Learning are: Instances are represented by attribute-value pairs. The target function has discrete output values.\n",
    "\n",
    "       i. Disjunctive descriptions may be required.\n",
    "      ii. The training data may contain errors.\n",
    "     iii. The training data may contain missing attribute values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e436377",
   "metadata": {},
   "source": [
    "## 16. Describe in depth the random forest model. What distinguishes a random forest?\n",
    "\n",
    "        The random forest is a classification algorithm consisting of many decisions trees. It uses bagging and feature randomness when building each individual tree to try to create an uncorrelated forest of trees whose prediction by committee is more accurate than that of any individual tree. The fundamental difference is that in Random forests, only a subset of features are selected at random out of the total and the best split feature from the subset is used to split each node in a tree, unlike in bagging where all features are considered for splitting a node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76b98ac",
   "metadata": {},
   "source": [
    "## 17. In a random forest, talk about OOB error and variable value ?\n",
    "\n",
    "        The out-of-bag (OOB) error is the average error for each calculated using predictions from the trees that do not contain in their respective bootstrap sample. There are two measures of importance given for each variable in the random forest. The first measure is based on how much the accuracy decreases when the variable is excluded.The second measure is based on the decrease of Gini impurity when a variable is chosen to split a node."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
